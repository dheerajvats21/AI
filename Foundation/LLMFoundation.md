# AI

## **Day 1: LLM Foundations**

- Topics:  
    **
  - **ML vs DL vs LLMs (high-level difference).  
        **
  - **Transformers: self-attention, why they scale well.  
        **
  - **Tokens & context window (important when talking about code).  
        **
- **Material:  
    **
  - **Video**
  - [**But what is a neural network? | Deep learning chapter 1**](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2) **‚Üí**[](https://www.youtube.com/watch?v=U0s0f995w14&utm_source=chatgpt.com)
  - [**Large Language Models explained briefly**](https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5)
  - [**Transformers, the tech behind LLMs | Deep Learning Chapter 5**](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6)

## **Day 2: Embeddings & Vector Search**

- **Topics:  
    **
  - **What embeddings are, how text/code ‚Üí vector.  
        **
  - **Semantic search vs keyword search.  
        **
  - **Why embeddings + vector DB are key for code analysis.  
        **
- **Material:  
    **
  - **Blog ‚Üí** [**OpenAI Embeddings Guide**](https://platform.openai.com/docs/guides/embeddings?utm_source=chatgpt.com)**.  
        **
  - **Blog ‚Üí Pinecone: What are Vector Embeddings?.  
        **

## **Day 3: Retrieval-Augmented Generation (RAG)**

- **Topics:  
    **
  - **Why you don‚Äôt fine-tune for every repo ‚Üí instead use RAG.  
        **
  - **Chunking code (functions/classes).  
        **
  - **Latency & retrieval accuracy challenges.  
        **
- **Material:  
    **
  - **Blog ‚Üí RAG Explained Simply.  
        **
  - **Example ‚Üí OpenAI Cookbook:** [**"Question answering with embeddings"**](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb?utm_source=chatgpt.com)**.  
        **

## **Day 4: Model Context Protocol (MCP)**

- **Topics:  
    **
  - **What MCP is (standard to connect IDE ‚Üî AI models).  
        **
  - **MCP servers expose tools (e.g., repo fetch, search).  
        **
  - **How IDE (like VSCode) can use MCP to visualize & extend context.  
        **
- **Material:  
    **
  - **Docs ‚Üí Model Context Protocol official site.  
        **
  - **Explore ‚Üí Anthropic‚Äôs MCP GitHub repos (reference implementations).  
        **

## **Day 5: AI in Developer Tools**

- **Topics:  
    **
  - **How AI is used in IDEs today (GitHub Copilot, Cody, Cursor).  
        **
  - **Limitations of current tools (context length, repo size).  
        **
  - **Where your project fits in (repo-aware MCP server).  
        **
- **Material:  
    **
  - **Blog ‚Üí How GitHub Copilot Works.  
        **
  - **Video ‚Üí ‚ÄúAI for Code‚Äù talk at Stanford (searchable on YouTube).  
        **

## **Day 6: Challenges & Trade-offs**

- **Topics:  
    **
  - **Token limits ‚Üí need chunking + retrieval.  
        **
  - **Latency ‚Üí caching embeddings.  
        **
  - **Cost ‚Üí avoid sending full repos to LLM.  
        **
  - **Security ‚Üí handling private repos, OAuth.  
        **
- **Material:  
    **
  - **Blog ‚Üí** [**LLM Limitations (OpenAI)**](https://platform.openai.com/docs/guides/limitations?utm_source=chatgpt.com)**.  
        **
  - **Short read ‚Üí Best practices for RAG.  
        **




## **üìå Cheat Sheet: ML vs DL vs LLMs**

**Machine Learning (ML)**

- **Algorithms that learn from data to make predictions (e.g., decision trees, regression, SVM).**
- **Doesn‚Äôt require deep neural networks.**
- **Example: Predicting house prices from features.  
    **

**Deep Learning (DL)**

- **Subset of ML using neural networks with many layers.**
- **Learns features automatically (no manual feature engineering).**
- **Example: Image recognition using CNNs, speech recognition using RNNs.**

**Large Language Models (LLMs)**

- **A special type of DL model (transformers) trained on massive text/code datasets.**
- **Learns statistical patterns in language.**
- **Can generate, summarize, translate, and reason about text/code.**
- **Examples: GPT, Claude, LLaMA.  
    **

**_‚ÄúMachine Learning is about algorithms that learn patterns from data, like regression or decision trees. Deep Learning is a subset that uses large neural networks to automatically extract features, making it powerful for images, speech, and text. Large Language Models are a specific kind of deep learning model, based on transformers, trained on massive amounts of text and code. They‚Äôre designed to work with natural language ‚Äî generating, summarizing, or reasoning about it.‚Äù_**

**_Token and context window_**

**_TOKENS - ‚ÄúLLMs process text as tokens, not characters. A token is a chunk of text like a word or part of a word. The context window is the max number of tokens the model can handle in one go. For example, GPT-4 can handle up to 128k tokens, which is about 300 pages. This limit is why you can‚Äôt just feed an entire codebase at once ‚Äî instead you split it into chunks and use embeddings + retrieval. Code gets tokenized too (operators, keywords, identifiers often become separate tokens).‚Äù_**

**_CONTEXT WINDOW- ‚ÄúIn GPT-3, the context window is ~4k tokens. At every step, the model predicts the next token based on everything in the current context. For conversations, the entire history is passed back in at each turn, so it doesn‚Äôt have true memory ‚Äî it just reprocesses the dialogue every time. Once you hit the 4k limit, older parts of the conversation are truncated, which is why long chats eventually lose earlier details. So it has working memory(short term) of 4k characters only allowing it maintain coherence‚Äù  
<br/>\- Multi-turn conversation - GPT doesn‚Äôt have memory between turns like a human brain. Instead, the conversation history is packed into the input every time:  
Turn 1: User: Hello Assistant: Hi, how can I help?  
Turn 2: Input to GPT is now the entire conversation so far: User: Hello Assistant: Hi, how can I help? User: Can you explain transformers? ‚Üí  
The model sees both the old and new text (as tokens) and predicts the next assistant reply._**

**LLM, Tech behind LLM, attention**
